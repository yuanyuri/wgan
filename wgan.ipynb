{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.rmtree(\"/home/ubuntu/wgan/model/\")\n",
    "os.mkdir(\"/home/ubuntu/wgan/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(object):\n",
    "    def __init__(self, input_width, input_height, output_width, output_height, z_dim, d_iters=5, g_iters=1):\n",
    "        self.z_dim = z_dim\n",
    "        self.d_iters = d_iters\n",
    "        self.g_iters = g_iters\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.output_width = output_width\n",
    "        self.output_height = output_height\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        with tf.name_scope(\"input\"):\n",
    "            self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n",
    "            self.imgs = tf.placeholder(tf.float32, [None, self.input_height, self.input_width, 3], name='real_images')\n",
    "\n",
    "        with tf.name_scope(\"generator_output\"):\n",
    "            self.g_img = self.generator(self.z)\n",
    "\n",
    "        with tf.name_scope(\"discriminate_loss\"):\n",
    "            d_real = self.discriminator(self.imgs)\n",
    "            d_fake = self.discriminator(self.g_img)\n",
    "           # Value Funciton, Discriminator最大化Value Function\n",
    "            self.d_loss = tf.reduce_mean(d_real) - tf.reduce_mean(d_fake)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"generator_loss\"):\n",
    "           # WGAN Generator 的损失函数\n",
    "            self.g_loss = -tf.reduce_mean(d_fake)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"optimizer_op\"):\n",
    "            self.d_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "            self.g_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generator\")\n",
    "            # WGAN 推荐采用 RMSPropOptimizer\n",
    "            self.d_optimizer = tf.train.RMSPropOptimizer(learning_rate=5e-5).minimize(-self.d_loss,\n",
    "                                                  var_list=self.d_trainable_vars)\n",
    "            # 对Discriminator weight clipping 近似满足1−𝐿𝑖𝑝𝑠𝑐ℎ𝑖𝑡𝑧\n",
    "            self.clip_d = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in self.d_trainable_vars]\n",
    "            self.g_optimizer = tf.train.RMSPropOptimizer(learning_rate=8e-5).minimize(\n",
    "                self.g_loss, var_list=self.g_trainable_vars)\n",
    "\n",
    "    def train(self, sess, batch_imgs, batch_z): \n",
    "        loss_d = 0\n",
    "        loss_g = 0\n",
    "        # 论文推荐Discriminator的更新轮数要大于Generator, 确保Discriminator 衡量真实分布与生成分布的差异更为准确\n",
    "        # 论文的更新次数为5\n",
    "        for i in range(self.d_iters):\n",
    "            loss_d, _, _ = sess.run([self.d_loss, self.d_optimizer, self.clip_d], feed_dict={self.imgs: batch_imgs, self.z:batch_z})\n",
    "        # 训练Generator 论文的更新次数为1\n",
    "        for i in range(self.g_iters):\n",
    "            loss_g, _ = sess.run([self.g_loss, self.g_optimizer], feed_dict={self.imgs: batch_imgs, self.z: batch_z})\n",
    "        return -loss_d, loss_g\n",
    "\n",
    "    def discriminator(self, img):\n",
    "        \"\"\"\n",
    "        区别于对图片进行分类(只需要抽象信息), discriminator的目的是判定图片的“真实性”\n",
    "        因此Discriminator不采用pooling 这样有损图片结构的操作\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "            d_layer_1 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(tf.contrib.layers.conv2d(img, num_outputs=64, activation_fn=None, kernel_size=[5, 5], stride=2, reuse=tf.AUTO_REUSE , weights_initializer=tf.random_normal_initializer(stddev=0.02), padding='SAME', scope='d_layer_1')))\n",
    "            d_layer_2 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(tf.contrib.layers.conv2d(d_layer_1, num_outputs=128, activation_fn=None, kernel_size=[5, 5], stride=2, reuse=tf.AUTO_REUSE, weights_initializer=tf.random_normal_initializer(stddev=0.02), padding='SAME', scope='d_layer_2')))\n",
    "            d_layer_3 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(tf.contrib.layers.conv2d(d_layer_2, num_outputs=256, activation_fn=None, kernel_size=[5, 5], stride=2, reuse=tf.AUTO_REUSE, weights_initializer=tf.random_normal_initializer(stddev=0.02), padding='SAME', scope='d_layer_3')))\n",
    "            d_layer_4 = tf.nn.leaky_relu(tf.contrib.layers.batch_norm(tf.contrib.layers.conv2d(d_layer_3,num_outputs=512,  activation_fn=None, kernel_size=[5, 5], stride=2, reuse=tf.AUTO_REUSE, weights_initializer=tf.random_normal_initializer(stddev=0.02), padding='SAME', scope='d_layer_4')))\n",
    "            flatten = tf.layers.flatten(d_layer_4)\n",
    "            logits = tf.layers.dense(flatten, 1, reuse=tf.AUTO_REUSE, name='d_layer_5', activation=None)\n",
    "        return logits\n",
    "\n",
    "    def generator(self, z):\n",
    "        \"\"\"\n",
    "        generator的目标是生成图片，如何从给定的向量去生成图片？\n",
    "        采用反卷积，将图片不断增大\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"generator\", reuse=False):\n",
    "            g_layer_1 = tf.nn.relu(tf.contrib.layers.batch_norm(tf.layers.dense(z, (self.input_width // 16) * (self.output_height // 16) * 512, activation=None, reuse=False)))\n",
    "            g_layer_1 = tf.reshape(g_layer_1, [-1, self.input_width // 16, self.output_height // 16, 512])\n",
    "            g_parameters = ( \n",
    "                            {\"num_outputs\": 256, \"kernel_size\": [3, 3], \"stride\": 2},\n",
    "                            {\"num_outputs\": 128, \"kernel_size\": [3, 3], \"stride\": 2},\n",
    "                            {\"num_outputs\": 64, \"kernel_size\": [3, 3], \"stride\": 2},\n",
    "                            )\n",
    "            x = g_layer_1\n",
    "            for parameters in g_parameters:\n",
    "                x = tf.nn.relu(tf.contrib.layers.batch_norm(tf.contrib.layers.conv2d_transpose(x, **parameters, activation_fn=None, reuse=False, padding='SAME', weights_initializer=tf.random_normal_initializer(stddev=0.02))))\n",
    "            img = tf.contrib.layers.conv2d_transpose(x, num_outputs=3, kernel_size=[3, 3], activation_fn=tf.nn.tanh, stride=2, reuse=False, padding='SAME', weights_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        return img\n",
    "\n",
    "    def infer(self, sess, model_dir, icon_dir, z_dim):\n",
    "        z_sample = np.random.normal(0, 1, [25, z_dim]).astype(np.float32)\n",
    "        #with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "        print(ckpt)\n",
    "        self.saver = tf.train.Saver(max_to_keep=10)\n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            print('Reloading model parameters...')\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(sess=sess, save_path=os.path.join(model_dir, ckpt_name))\n",
    "        samples = sess.run(self.g_img, feed_dict={self.z: z_sample})\n",
    "        save_images(samples, icon_dir)\n",
    "\n",
    "    def visualize_result(self, samples):\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        gs = gridspec.GridSpec(5, 5)\n",
    "        gs.update(wspace=0.1, hspace=0.1)\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample)\n",
    "        return fig\n",
    "\n",
    "def save_images(images, image_path):\n",
    "    gen_imgs = inverse_transform(images)\n",
    "    r, c = 5, 5\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(image_path + \"icon1.png\")\n",
    "    plt.close()\n",
    "\n",
    "def inverse_transform(images):\n",
    "    #return  (images + 1.) / 2.\n",
    "    return np.array(((images+1.)/2.)*255., dtype=int)\n",
    "\n",
    "def get_batch(path, batch_size, z_dim):\n",
    "    files = os.listdir(path)\n",
    "    np.random.shuffle(files)\n",
    "    batch_imgs = []\n",
    "    for file in files:\n",
    "        img = np.array(Image.open(path + file).resize((64, 64)))\n",
    "        img = (img / 255.) * 2. - 1\n",
    "        batch_imgs.append(img)\n",
    "        if len(batch_imgs) == batch_size:\n",
    "            batch_z = np.random.normal(0, 1, [batch_size, z_dim]).astype(np.float32)\n",
    "            yield batch_imgs, batch_z\n",
    "            batch_imgs = []\n",
    "    if len(batch_imgs) != 0:\n",
    "        batch_z = np.random.normal(0, 1, [len(batch_imgs), z_dim]).astype(np.float32)\n",
    "        yield batch_imgs, batch_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = \"/home/ubuntu/wgan/faces/\"\n",
    "path = \"/home/ubuntu/wgan\"\n",
    "pic_path = \"/home/ubuntu/wgan\"\n",
    "model_dir = \"/home/ubuntu/wgan/model/\"\n",
    "train_batch_size = 128\n",
    "epochs = 102\n",
    "z_dim = 100\n",
    "g = tf.Graph()\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "d_loss = 0\n",
    "g_loss = 0\n",
    "with g.as_default():\n",
    "    wgan = WGAN(input_height=64, input_width=64, output_height=64, output_width=64, z_dim=z_dim)\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        wgan.build_model()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        saver = tf.train.Saver(max_to_keep=10)\n",
    "        ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "        # 读取model的存档,如果有存档，从存档点开始接着训练\n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            print('Reloading model parameters...')\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess=sess, save_path=os.path.join(model_dir, ckpt_name))\n",
    "        for epoch_id in range(epochs):\n",
    "            start = time.time()\n",
    "            d_loss = 0\n",
    "            g_loss = 0\n",
    "            train_data = get_batch(path=img_path, batch_size=train_batch_size, z_dim=z_dim)\n",
    "            for batch_imgs, batch_z in train_data:\n",
    "                d_loss_batch, g_loss_batch = wgan.train(sess, batch_imgs, batch_z)\n",
    "                d_loss += d_loss_batch\n",
    "                g_loss += g_loss_batch\n",
    "            end = time.time()\n",
    "            print(\"Epoch %d / %d, \" % (epoch_id + 1, epochs),\n",
    "                  \"Discriminator Train Loss: %.4f\" % d_loss,\n",
    "                  \"Generator Train Loss: %.4f\" % g_loss,\n",
    "                  \"%.4f sec / epoch\" % ((end - start)))\n",
    "            saver.save(sess, model_dir, global_step=epoch_id)\n",
    "        wgan.infer(sess, model_dir=model_dir, icon_dir=pic_path, z_dim=z_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
